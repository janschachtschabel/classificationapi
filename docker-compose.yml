version: '3.8'

services:
  classificationapi:
    build: .
    ports:
      - "8000:8000"
    environment:
      # Override with your actual OpenAI API key
      - OPENAI_API_KEY=${OPENAI_API_KEY}
      - OPENAI_DEFAULT_MODEL=gpt-4o-mini
      - API_HOST=0.0.0.0
      - API_PORT=8000
      - API_DEBUG=false
      - LOG_LEVEL=INFO
      - LOG_FORMAT=json
      - CACHE_TTL_SECONDS=3600
      - CACHE_MAX_SIZE=1000
      - RATE_LIMIT_REQUESTS_PER_MINUTE=60
      - HTTP_TIMEOUT_SECONDS=30
      - OPENAI_TIMEOUT_SECONDS=60
    volumes:
      # Mount .env file for local development (optional)
      - ./.env:/app/.env:ro
      # Mount logs directory for persistence (optional)
      - ./logs:/app/logs
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8000/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 40s
    restart: unless-stopped
    networks:
      - classificationapi-network

  # Optional: Add a reverse proxy for production
  # nginx:
  #   image: nginx:alpine
  #   ports:
  #     - "80:80"
  #     - "443:443"
  #   volumes:
  #     - ./nginx.conf:/etc/nginx/nginx.conf:ro
  #     - ./ssl:/etc/nginx/ssl:ro
  #   depends_on:
  #     - classificationapi
  #   networks:
  #     - classificationapi-network

networks:
  classificationapi-network:
    driver: bridge

# Optional: Add volumes for data persistence
# volumes:
#   logs:
#   cache:
